import torch
import torch.nn as nn
import coremltools as ct
import numpy as np
from transformers import EomtForUniversalSegmentation, AutoImageProcessor

# ---------------------------------------------------------------------------
# 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
# ---------------------------------------------------------------------------
model_id = "tue-mps/coco_panoptic_eomt_small_640_2x"
print(f"ğŸ“¥ Loading model & processor: {model_id}...")

# Processorì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
processor = AutoImageProcessor.from_pretrained(model_id)
base_model = EomtForUniversalSegmentation.from_pretrained(model_id)
base_model.eval()

# ---------------------------------------------------------------------------
# 2. [í•µì‹¬] ImageNet Mean/Std ì—­ì‚°í•˜ì—¬ Core ML íŒŒë¼ë¯¸í„° ê³„ì‚°
# ---------------------------------------------------------------------------
# PyTorch ê³µì‹: output = (image/255.0 - mean) / std
# Core ML ê³µì‹: output = (image * scale) + bias
# ë”°ë¼ì„œ:
# scale = 1 / (255.0 * std)
# bias  = -mean / std

image_mean = np.array(processor.image_mean) # [0.485, 0.456, 0.406]
image_std = np.array(processor.image_std)   # [0.229, 0.224, 0.225]

print(f"ğŸ“Š Processor Mean: {image_mean}")
print(f"ğŸ“Š Processor Std : {image_std}")

# Core ML ImageTypeì˜ scaleì€ ë‹¨ì¼ float ê°’ë§Œ í—ˆìš©ë˜ëŠ” ê²½ìš°ê°€ ë§ìŒ (ì±„ë„ë³„ ì°¨ì´ê°€ í¬ì§€ ì•Šìœ¼ë¯€ë¡œ í‰ê·  ì‚¬ìš©)
# ë¯¸ì„¸í•œ ì°¨ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•´ bias ê³„ì‚° ì‹œì—ëŠ” ê° ì±„ë„ë³„ stdë¥¼ ë°˜ì˜
avg_std = np.mean(image_std) 

scale = 1.0 / (255.0 * avg_std)
bias = (-image_mean / image_std).tolist() # RGB ì±„ë„ë³„ Bias

print(f"ğŸ§® Calculated Scale: {scale}")
print(f"ğŸ§® Calculated Bias : {bias}")

# ---------------------------------------------------------------------------
# 3. Wrapper (Dict -> Tuple)
# ---------------------------------------------------------------------------
class EomtWrapper(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
    
    def forward(self, x):
        # xëŠ” Core MLì´ ì „ì²˜ë¦¬í•œ Tensorê°€ ë“¤ì–´ì˜´
        outputs = self.model(pixel_values=x)
        return outputs.class_queries_logits, outputs.masks_queries_logits

wrapper_model = EomtWrapper(base_model)

# ---------------------------------------------------------------------------
# 4. Tracing
# ---------------------------------------------------------------------------
# Traceìš© ë”ë¯¸ ì…ë ¥ (ê°’ì€ ìƒê´€ì—†ìŒ, í˜•íƒœë§Œ ì¤‘ìš”)
dummy_input = torch.rand(1, 3, 640, 640)
print("ğŸ¥ Tracing model...")
traced_model = torch.jit.trace(wrapper_model, dummy_input)

# ---------------------------------------------------------------------------
# 5. Core ML ë³€í™˜ (ì •í™•ë„ ìµœìš°ì„  ì„¤ì •)
# ---------------------------------------------------------------------------
print("ğŸ“¦ Converting to Core ML Package...")

model_ct = ct.convert(
    traced_model,
    inputs=[
        ct.ImageType(
            name="pixel_values", 
            shape=(1, 3, 640, 640), # ëª¨ë¸ì˜ í•™ìŠµ í•´ìƒë„
            scale=scale, 
            bias=bias,
            color_layout=ct.colorlayout.RGB # ëª…ì‹œì  RGB ì§€ì •
        )
    ],
    outputs=[
        ct.TensorType(name="class_logits"),
        ct.TensorType(name="mask_logits")
    ],
    minimum_deployment_target=ct.target.iOS16,
    convert_to="mlprogram",
    
    # ğŸ‘‡ [ê°€ì¥ ì¤‘ìš”] ê²°ê³¼ê°€ ì•ˆ ì¢‹ì€ ê²°ì •ì  ì›ì¸ í•´ê²° (FP32 ê°•ì œ)
    compute_precision=ct.precision.FLOAT32
)

# ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì„ íƒì‚¬í•­)
model_ct.user_defined_metadata["com.apple.coreml.model.preview.type"] = "imageSegmenter"
model_ct.short_description = "EOMT Panoptic Segmentation (FP32)"

save_path = "EOMT_2.mlpackage"
model_ct.save(save_path)
print(f"âœ… Success! Saved '{save_path}' with FLOAT32 precision.")